{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4bd7482-2388-4f8c-a550-cb70adaadf93",
   "metadata": {},
   "source": [
    "# Toxicity Prediction with Discovery Workbench (DWb)\n",
    "\n",
    "**Required libraries:** torch, pytorch-fast-transformers, pytorch-lightning, transformers, scipy, scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecbb64e9-2bd2-469f-beac-a811444745e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kbcomposer import DWB, KBComposer\n",
    "\n",
    "import torch\n",
    "from models.molformer_predict_tox import LightningModule\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "from helper import dotdict\n",
    "from helper import convert_to_mgkg\n",
    "from helper import convert_to_epa\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from tokenizer import MolTranBertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9dbf70e-569f-4b07-9649-73a296ccfd89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuring DWb's instance\n",
    "kbc = KBComposer.get_kbc()\n",
    "dwb = DWB.get_dwb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3716894b-589d-4e97-9de5-ede718a15712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to load datasets from DWb using Pandas\n",
    "def download_semantic_dataset_by_filename(dwb, filename):\n",
    "    ds = [x for x in dwb.get_semantic_datasets() if x['label'] == filename][0]\n",
    "    return dwb.download_semantic_dataset(ds['uri'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78421d9f-736a-497a-8ffa-0168aeaf31d2",
   "metadata": {},
   "source": [
    "## Upload a semantic dataset to DWb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7d641e3-3892-4683-9f7e-f2e493acc7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additionalAttributes': ['Unnamed: 0',\n",
      "                          'BCUTi-1h',\n",
      "                          'BCUTd-1h',\n",
      "                          'TopoPSA(NO)',\n",
      "                          'BCUTs-1l',\n",
      "                          'VR3_D',\n",
      "                          'SMR_VSA1',\n",
      "                          'BCUTv-1h',\n",
      "                          'SlogP_VSA2',\n",
      "                          'BCUTd-1l',\n",
      "                          'BCUTc-1h',\n",
      "                          'SMR_VSA5',\n",
      "                          'SdsssP',\n",
      "                          'IC0',\n",
      "                          'BCUTm-1l',\n",
      "                          'Mv',\n",
      "                          'BCUTi-1l',\n",
      "                          'EState_VSA1',\n",
      "                          'BCUTc-1l',\n",
      "                          'TopoPSA',\n",
      "                          'PEOE_VSA8',\n",
      "                          'BCUTdv-1h',\n",
      "                          'Xch-7d',\n",
      "                          'MID_h',\n",
      "                          'MDEC-23',\n",
      "                          'MDEC-33',\n",
      "                          'Xch-6dv',\n",
      "                          'BCUTm-1h',\n",
      "                          'BCUTv-1l',\n",
      "                          'AMID_N',\n",
      "                          'Class',\n",
      "                          'ld50'],\n",
      " 'authorUri': '<http://brl.ibm.com/ontologies/2020/coreAIWB_ABox#dmljdG9yLnNoaXJhc3VuYUBpYm0uY29t>',\n",
      " 'committed': False,\n",
      " 'count': 2666,\n",
      " 'creationDate': '2023-05-04T12:22:05.756Z',\n",
      " 'description': 'Test dataset for toxicity prediction.',\n",
      " 'label': 'Toxicity prediction test',\n",
      " 'membersInformation': {'<http://purl.obolibrary.org/obo/CHEBI_25367>': {'count': 2666}},\n",
      " 'membersTypeUris': ['<http://purl.obolibrary.org/obo/CHEBI_25367>'],\n",
      " 'type': 'semantic',\n",
      " 'uri': '<hk://id/1683202925_755969_10c9838b_58e3_4de9_a883_b1fa6021f0ad>'}\n"
     ]
    }
   ],
   "source": [
    "# Select molecule concept\n",
    "concepts = dwb.get_concepts()\n",
    "concept = next(filter(lambda c: c['label'] == 'Molecule', concepts))\n",
    "concept_uri = concept['uri']\n",
    "\n",
    "# Upload semantic dataset\n",
    "df = pd.read_csv('data/toxicity-prediction/toxicity-prediction_test_dwb.csv')\n",
    "semantic_ds = dwb.upload_semantic_dataset(dataset_label='Toxicity prediction test',\n",
    "                                          dataset_description='Test dataset for toxicity prediction.',\n",
    "                                          member_type=concept_uri,\n",
    "                                          dataframe=df,\n",
    "                                          label_column='SMILES')\n",
    "pprint(dwb.get_semantic_dataset_info(semantic_ds['uri']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a948b4c6-676b-4583-8c21-f14c42df6596",
   "metadata": {},
   "source": [
    "## Inference function\n",
    "\n",
    "Model checkpoint can be downloaded from [box](https://ibm.ent.box.com/folder/201653400157)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fac9ad88-924a-4596-8f7c-d599c2921437",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference():\n",
    "    \n",
    "    print('Import model.')\n",
    "    \n",
    "    # Network loading and parameters importing\n",
    "    with open('data/hparams.yaml') as f:\n",
    "        data = yaml.load(f, Loader=SafeLoader)\n",
    "        print(data)\n",
    "\n",
    "    hparams = dotdict(data)\n",
    "    tokenizer = MolTranBertTokenizer('data/bert_vocab.txt')\n",
    "    model = LightningModule(hparams, tokenizer).load_from_checkpoint('data/last.ckpt',\n",
    "                                                                     strict=False,\n",
    "                                                                     config=hparams,\n",
    "                                                                     tokenizer=tokenizer,\n",
    "                                                                     vocab=len(tokenizer.vocab),\n",
    "                                                                     map_location=torch.device('cpu'))\n",
    "    model.eval()\n",
    "    \n",
    "    print('Retrieving semantic dataset from DWb.')\n",
    "    \n",
    "    # Importing data and transforming to the Network \n",
    "    #df = pd.read_csv('data/toxicity-prediction/toxicity-prediction_test.csv', nrows=20)\n",
    "    df = download_semantic_dataset_by_filename(dwb, 'Toxicity prediction test')\n",
    "    df = df.iloc[:20]\n",
    "    \n",
    "    print('Data transformation.')\n",
    "\n",
    "    # Tokenizer - Creating tokens from SMILES\n",
    "    tokens = model.tokenizer(df['SMILES'].tolist(), padding=True, truncation =True, add_special_tokens=True,return_tensors=\"pt\" )\n",
    "    idx = torch.tensor(tokens['input_ids'])\n",
    "    mask = torch.tensor(tokens['attention_mask'])\n",
    "\n",
    "    # Data transformation to feed the model\n",
    "    token_embeddings = model.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "    x = model.drop(token_embeddings)\n",
    "    x = model.blocks(x, length_mask=LM(mask.sum(-1)))\n",
    "    token_embeddings = x\n",
    "\n",
    "    input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    loss_input = sum_embeddings / sum_mask\n",
    "\n",
    "    outmap_min, _ = torch.min(loss_input, dim=1, keepdim=True)\n",
    "    outmap_max, _ = torch.max(loss_input, dim=1, keepdim=True)\n",
    "    outmap = (loss_input - outmap_min) / (outmap_max - outmap_min) # Broadcasting rules apply\n",
    "    \n",
    "    print('Predicting...')\n",
    "    \n",
    "    outputs = model.net.forward(outmap).squeeze()\n",
    "    \n",
    "    # Converting to Epa Categories\n",
    "    pred_epa = list(convert_to_epa(outputs.squeeze(),df['SMILES']))\n",
    "    # Converting to Mg/Kg  Units\n",
    "    pred_epa_mgkg = list(convert_to_mgkg(outputs.squeeze(),df['SMILES']))\n",
    "\n",
    "    print(list(zip(pred_epa, pred_epa_mgkg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7277271-f710-4c7c-92d7-cb8cb23ace8b",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe0f853d-f0eb-4b90-a9f5-a4307a043d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import model.\n",
      "{'aug': None, 'batch_size': 32, 'checkpoint_dir': './checkpoints_toxicity-causal-epa/ld50/models', 'checkpoint_every': 1000, 'checkpoint_root': './checkpoints_toxicity-causal-epa/ld50', 'checkpoints_folder': './checkpoints_toxicity-causal-epa', 'd_dropout': 0.1, 'data_root': '../data/toxicity-prediction-causal', 'dataset_name': 'toxicity-prediction', 'dataset_names': ['valid', 'test'], 'desc_skip_connection': False, 'device': 'cuda', 'dims': [768, 768, 768, 1], 'dropout': 0.1, 'eval_dataset_length': None, 'fc_h': 512, 'fold': 0, 'from_scratch': False, 'lr_multiplier': 1, 'lr_start': 3e-05, 'max_epochs': 2000, 'measure_name': 'ld50', 'mode': 'avg', 'n_batch': 512, 'n_embd': 768, 'n_head': 12, 'n_jobs': 1, 'n_layer': 12, 'num_classes': None, 'num_feats': 32, 'num_workers': 8, 'results_dir': './checkpoints_toxicity-causal-epa/ld50/results', 'run_name': 'toxicity-prediction_ld50_rot_0_avg_lr_3e-05_batch_32_drop_0.1_[768, 768, 768, 1]', 'seed': 12345, 'seed_path': '../data/checkpoints/N-Step-Checkpoint_3_30000.ckpt', 'smiles_embedding': '/dccstor/medscan7/smallmolecule/runs/ba-predictor/small-data/embeddings/protein/ba_embeddings_tanh_512_2986138_2.pt', 'train_dataset_length': None}\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "dropout is 0.1\n",
      "smiles_embed_dim:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.1.5 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file data/last.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "dropout is 0.1\n",
      "smiles_embed_dim:  768\n",
      "Retrieving semantic dataset from DWb.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transformation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1203/10746566.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  idx = torch.tensor(tokens['input_ids'])\n",
      "/tmp/ipykernel_1203/10746566.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(tokens['attention_mask'])\n",
      "/usr/local/lib/python3.9/site-packages/fast_transformers/feature_maps/fourier_features.py:37: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2425.)\n",
      "  Q, _ = torch.qr(block)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "[(2, 1439.8095763700335), (2, 2705.3795426443676), (2, 3617.1633299139435), (2, 1380.1931552040612), (1, 111.24934474181616), (2, 1453.3453255839745), (1, 423.8074093303452), (2, 3747.3138012803192), (1, 178.36611823361258), (2, 2885.6612386331763), (2, 650.359590992821), (2, 564.3009972254724), (1, 484.3250089075924), (1, 346.4622882608363), (1, 309.9638203285186), (2, 923.5172480555726), (2, 2098.4612331892195), (2, 1002.6421872697192), (2, 1373.438963361423), (3, 7929.463927319171)]\n",
      " Total time = 1259.7 seconds. Total time = 21.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load\n",
    "inference()\n",
    "\n",
    "total_time = time.time() - start_time        \n",
    "if(total_time > 60):\n",
    "    print(f\" Total time = {total_time:.1f} seconds. Total time = {(total_time/60):.1f} minutes.\")\n",
    "else: \n",
    "    print(f\" Total time = {total_time:.1f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
